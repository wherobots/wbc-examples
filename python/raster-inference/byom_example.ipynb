{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37fe0da",
   "metadata": {},
   "source": [
    "![](https://wherobots.com/wp-content/uploads/2023/12/Inline-Blue_Black_onWhite@3x.png)\n",
    "\n",
    "## WherobotsAI Raster Inference - Bring Your Own Model\n",
    "\n",
    "This example demonstrates how to bring your own model to Raster Inference. We will start by describing a machine learning model from [Satlas](https://satlas.allen.ai/ai) <sup>1</sup> using the Machine Learning Model STAC Extension (MLM). After we have an MLM JSON file desribig our model on S3, we can pass this to RS_* inference functions to run our model inference on Wherobots Cloud. \n",
    "\n",
    "**Note: This notebook requires the Wherobots Inference functionality to be enabled and a GPU runtime selected in Wherobots Cloud. Please [contact us](https://wherobots.com/contact/) to enable these features.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9444249",
   "metadata": {},
   "source": [
    "### Step 1: Creating the MLM metadata with `stac_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efbf28",
   "metadata": {},
   "source": [
    "The MLM specification is what we use to define the model inference requirements. These include descriptions of model inputs (their shape, role, and preprocessing steps), the categories associated with a model, the model task, and the location of the model asset. Wherobots maintains the `stac-model` python library which can be used to create and validate that metadata complies with the MLM specification requirements.\n",
    "\n",
    "Below we will break down the steps involved to fill out and validate the MLM fields using `stac-model`. But first we will save the model artifact to our s3 user path that is referred by the MLM metadata so that we can later run inference with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfaf29-6e47-4218-b7d4-d038bda698db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "\n",
    "user_uri = os.getenv(\"USER_S3_PATH\")\n",
    "original_model_uri = \"s3://wherobots-modelhub-prod/professional/semantic-segmentation/solar-satlas-sentinel2/inductor/gpu/aot_inductor_gpu_tensor_cores.zip\"\n",
    "user_model_uri = f\"{user_uri}aot_inductor_gpu_tensor_cores.zip\"\n",
    "user_mlm_uri = f\"{user_uri}model-metadata.json\"\n",
    "\n",
    "fs = fsspec.filesystem('s3')\n",
    "fs.copy(original_model_uri, user_model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e429c",
   "metadata": {},
   "source": [
    "The main libary we will use to construct the metadata is `stac-model`, which implements validation for the MLM fields, which begin with `mlm:`.\n",
    "\n",
    "We also use `pystac` to combine MLM metadata with other STAC core and STAC extension metadata. For a primer on STAC and STAC extensions, check out https://stac-extensions.github.io/, which also lists the many extensions out there for describing different kinds of spatio-temporal data.\n",
    "\n",
    "The other libraries, `shapely` and `dateutil` are briefly used to format geometry and time metadata in our metadata JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c33677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac\n",
    "import shapely\n",
    "from dateutil.parser import parse as parse_dt\n",
    "\n",
    "from stac_model.base import ProcessingExpression\n",
    "from stac_model.input import InputStructure, ModelInput, MLMStatistic\n",
    "from stac_model.output import MLMClassification, ModelOutput, ModelResult\n",
    "from stac_model.schema import MLModelExtension, MLModelProperties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7568793",
   "metadata": {},
   "source": [
    "The [InputStructure object](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#input-structure-object) describes the shape of a tensor/array input to a model's predict function. This describes the shape of the input after all data processing steps have been applied to the original data input.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    " We'll refer to arrays as tensors throughout this guide. A tensor is just a fancy name for raster array that is used in a machine learning model. Since Pytorch is the most popular machine learning framework, we will use Pytorch's terminology. Pytorch tensors are similar to Numpy arrays and conversion between the two is easy.\n",
    "</div>\n",
    "\n",
    "For our example SATLAS model, the expected input structure is a tensor with a flexible batch size (`-1`), `9` bands multiplied by `4` time steps to form a single dimension, and `1024` height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441015cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = InputStructure(\n",
    "    shape=[-1, 9 * 4, 1024, 1024], dim_order=[\"batch\", \"channel\", \"height\", \"width\"], data_type=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54feac45",
   "metadata": {},
   "source": [
    "The [MLMStatistic object](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#bands-and-statistics) describes the statistics for the input to a model's prediction function. Typically this describes statistics that are necessary for normalizing the model input to the data range and/or the data distribution expected by the model. Some models may only need the data range adjusted and will handle normalizing the inputs to a given distribution, others will adjust both the range and distribution. The field [`norm_type`](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#normalize-enum) indicates how inputs need to be normalized given the statistics.\n",
    "\n",
    "Wherobots Raster Inference functions expect that the statistics are applied to the band dimension of an overhead imagery input. `band_names` should be specified according to the STAC Collection that the model input expects. For interoperability with WherobotsAI Raster Inference, use strings for band names rather than the [Model Band Object](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#model-band-object).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If Statistics are specified for an overhead imagery input with bands, we recommend specifying the statistics for each band, even if the statistics are the same in order to remove ambiguity about Statistics meaning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290aa5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_names = [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\"]*4\n",
    "stats = [\n",
    "    MLMStatistic(maximum=255, minimum=0)\n",
    "    for band in band_names\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd30772",
   "metadata": {},
   "source": [
    "With the band names, input structure (`input_array`), and statistics, we have a few other fields to fill out to fully describe our [Model Input](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#model-input-object). `norm_by_channel` specifies if Statistics should be applied per channel/band, or if Statistics should be applied to the input_array as a whole. If it is set to True, Statistics must be the same length as the band dimension of the `input_array`. `norm_type` specifies the normalization equation to apply to the `input_array` and `Statistics`. The `resize_type` indicates how to convert data samples to the size expected by the model's predict function, which is specified by the [InputStructure object](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#input-structure-object). For processing operations that cannot be described by the normalization and resize_type fields, the `pre_processing_function` can be used to either link to documentation or specify the mathematical expresson to apply to model inputs before prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38509d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = ModelInput(\n",
    "    name=\"9 Band Sentinel-2 4 Time Step Series Batch\",\n",
    "    bands=band_names,\n",
    "    input=input_array,\n",
    "    norm_by_channel=True,\n",
    "    norm_type=\"min-max\",\n",
    "    resize_type=\"crop\",\n",
    "    statistics=stats,\n",
    "    pre_processing_function=ProcessingExpression(format=\"documentation-link\", expression=\"https://github.com/allenai/satlas/blob/main/CustomInference.md#sentinel-2-inference-example\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61264dd1",
   "metadata": {},
   "source": [
    "Similar to the [Input Structure object](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#input-structure-object), we need to describe the model output with the [Result Structure object](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#result-structure-object). `-1` denotes a flexible batch size, `1` refers to the category dimension (in this case the model predicts only solar farm/not solar farm), and `1024` refers to our height and width dimensions again. `dim_order` enumerates the meaning of each shape element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eeef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = ModelResult(\n",
    "    shape=[-1, 1, 1024, 1024],\n",
    "    dim_order=[\"batch\", \"category\", \"height\", \"width\"],\n",
    "    data_type=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d1325",
   "metadata": {},
   "source": [
    "We use the [STAC Classification Extension](https://github.com/stac-extensions/classification) to describe categories predicted by the model. The only required fields are an integer value to represent the category and the name of the category.\n",
    "\n",
    "The [Model Output Object](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#model-output-object) ties together the classes, task, and any post processing functions that need to be applied to the Result Structure Object. Options for tasks are specified in the [Tasks Enum](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#task-enum). WherobotsAI Raster Inference currently supports `scene-classification`, `object-detection`, and `semantic-segmentation`.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  Let us know what other tasks and models you would like to see us support at <a href=\"https://community.wherobots.com/\">https://community.wherobots.com/</a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"Solar Farm\": 1,}\n",
    "class_objects = [\n",
    "    MLMClassification(value=class_value, name=class_name)\n",
    "    for class_name, class_value in class_map.items()\n",
    "]\n",
    "model_output = ModelOutput(\n",
    "    name=\"confidence array\",\n",
    "    tasks={\"semantic-segmentation\"},\n",
    "    result=confidence,\n",
    "    classes=class_objects,\n",
    "    post_processing_function=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f4def",
   "metadata": {},
   "source": [
    "To describe the actual model file we load and run in Raster Inference, we use the STAC Core spec for [Asset Objects](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#assets-objects). The MLM spec adds aditional fields and roles for different asset types. To use the MLM you are required to specify the `mlm:model` asset that points to a model file and where it is hosted (`href`). The [Artifact Enum](\"https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#artifact-type-enum\") specifies the type of the model file. Currently Pytorch `torch.jit.script` and `torch.compile` models are supported by Raster Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = {\n",
    "    \"model\": pystac.Asset(\n",
    "        title=\"AOTInductor model exported from private, edited, hard fork of Satlas github repo.\",\n",
    "        description=(\n",
    "            \"A Swin Transformer backbone with a U-net head trained on the 9-band Sentinel-2 Top of Atmosphere product.\"\n",
    "        ),\n",
    "        href=user_model_uri,\n",
    "        media_type=\"application/zip; application=pytorch\",\n",
    "        roles=[\n",
    "            \"mlm:model\",\n",
    "            \"data\"\n",
    "        ],\n",
    "        extra_fields={\"mlm_artifact_type\": \"torch.compile\",}\n",
    "    ),\n",
    "    \"source_code\": pystac.Asset(\n",
    "        title=\"Model implementation.\",\n",
    "        description=\"Source code to export the model.\",\n",
    "        href=\"https://github.com/wherobots/modelhub/blob/main/model-forge/satlas/solar/export.py\",\n",
    "        media_type=\"text/x-python\",\n",
    "        roles=[\n",
    "            \"mlm:model\",\n",
    "            \"code\"\n",
    "        ]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48063ae",
   "metadata": {},
   "source": [
    "After specifying our model input, model output, and assets, we can assemble this info in the top level [Item Properties](https://github.com/crim-ca/mlm-extension?tab=readme-ov-file#item-properties-and-collection-fields). Note that the required fields of the spec are `mlm:name`, `mlm:architecture`, `mlm:tasks`, `mlm:input`, and `mlm:output`. Additionally, WherobotsAI Raster Inference requires the following fields and options: \n",
    "\n",
    "1. Only one value for `tasks` is supported.\n",
    "1. `framework=\"pytorch\"` is currently required\n",
    "2. `framework_version=\"2.3.0+cu121\"` is recommended as this is the default version installed in our GPU Runtimes. You can still install a different version during Notebook Instance setup if needed, however Raster Inference expects to run on features availabler in Pytorch 2.3.\n",
    "3. `batch_size_suggestion` is recommended. If not specified, Raster Inference defaults to a batch size of 10, or the batch size that is set in the sedona configuration `wherobots.inference.args`. For example: \n",
    "\n",
    "```python\n",
    "    config = (\n",
    "        SedonaContext.builder()\n",
    "        .appName(\"raster-inference\")\n",
    "        .config(\"spark.wherobots.inference.args\", \"10\") # sets the batch size for RS_ inference functions to 10\n",
    "```\n",
    "4. `accelerator` is recommended. If not set, we assume CUDA is available.\n",
    "5. `accelerator_constrained` is recommended to indicate that a model must run on the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3568801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model_meta = MLModelProperties(\n",
    "    name=\"Satlas Solar Farm Segmentation\",\n",
    "    architecture=\"Swin Transformer V2 with U-Net head\",\n",
    "    tasks={\"semantic-segmentation\"},\n",
    "    framework=\"pytorch\",\n",
    "    framework_version=\"2.3.0+cu121\",\n",
    "    batch_size_suggestion=10,\n",
    "    accelerator=\"cuda\",\n",
    "    accelerator_constrained=True,\n",
    "    accelerator_summary=\"It is necessary to use GPU since it was compiled for NVIDIA Ampere and newer architectures with AOTInductor and the computational demands of the model.\",\n",
    "    file_size=333_000_000,\n",
    "    memory_size=1,\n",
    "    pretrained=True,\n",
    "    pretrained_source=\"Sentinel-2 imagery and SATLAS labels\",\n",
    "    total_parameters=89_748_193,\n",
    "    input=[model_input],\n",
    "    output=[model_output],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee94da",
   "metadata": {},
   "source": [
    "A requirement of describing the model with the MLM is specifying it's spatial and temporal relevance. These fields are not used by Raster Inference currently but can be useful for search and discovery. These fields must have a value to comply with STAC.\n",
    "\n",
    "After assembling all of our model metadata we now need to create a STAC item with `pystac` where we will insert our MLM Extension metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime_str = \"1900-01-01\"\n",
    "end_datetime_str = \"9999-01-01\"\n",
    "start_datetime = parse_dt(start_datetime_str).isoformat() + \"Z\"\n",
    "end_datetime = parse_dt(end_datetime_str).isoformat() + \"Z\"\n",
    "bbox = [\n",
    "    -7.882190080512502,\n",
    "    37.13739173208318,\n",
    "    27.911651652899923,\n",
    "    58.21798141355221\n",
    "]\n",
    "geometry = shapely.geometry.Polygon.from_bounds(*bbox).__geo_interface__\n",
    "item_name = \"item_solar_satlas_sentinel2\"\n",
    "\n",
    "item = pystac.Item(\n",
    "    id=item_name,\n",
    "    geometry=geometry,\n",
    "    bbox=bbox,\n",
    "    datetime=None,\n",
    "    properties={\n",
    "        \"start_datetime\": start_datetime,\n",
    "        \"end_datetime\": end_datetime,\n",
    "        \"description\": (\n",
    "            \"Sourced from satlas source code released by Allen AI under Apache 2.0\"\n",
    "        ),\n",
    "    },\n",
    "    assets=assets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77031d04",
   "metadata": {},
   "source": [
    "We add a link to the source dataset that the model was trained on and should be inferenced on. If the model is trained on multiple datasets, multiple links can be added with the `DERIVED_FROM` relation type. We also add a self referential link to the Item to aid in search and discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a67e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "item.add_link(\n",
    "    pystac.Link(\n",
    "        target=\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l1c\",\n",
    "        rel=pystac.RelType.DERIVED_FROM,\n",
    "        media_type=pystac.MediaType.JSON,\n",
    "    )\n",
    ")\n",
    "item.set_self_href(user_mlm_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0823b",
   "metadata": {},
   "source": [
    "Finaly we add our extension metadata to the item we created with `pystac`. Using the `.ext()` method we produce an item that has all of the methods from `pystac` as well as custom MLM methods that are needed to correctly format and validate MLM metadata with `.apply()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce56532",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mlm = MLModelExtension.ext(item, add_if_missing=True)\n",
    "item_mlm.apply(ml_model_meta.model_dump(by_alias=True, exclude_unset=False, exclude_defaults=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97ddf6",
   "metadata": {},
   "source": [
    "This can now be saved to a JSON file and copied to the `user_mlm_uri` path we specified on s3. We will also need to copy our model artifact path to the correct link we specified in the model asset object in order to use the model in Raster Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"model-metadata.json\", \"w\") as json_file:\n",
    "    json.dump(item_mlm.item.to_dict(), json_file, indent=4)\n",
    "fs.put(\"model-metadata.json\", user_mlm_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fec939-6e1a-486a-93ec-d6ac7a7b9aaa",
   "metadata": {},
   "source": [
    "### Step 1: Set Up The WherobotsDB Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from wherobots.inference.data.io import read_raster_table\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "config = SedonaContext.builder().appName('segmentation-batch-inference')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654ce00-87ec-4200-88ac-feac99575490",
   "metadata": {},
   "source": [
    "### 2: Load Satellite Imagery\n",
    "\n",
    "Next, we load the satellite imagery that we will be running inference over. These GeoTiff images are loaded as *out-db* rasters in WherobotsDB, where each row represents a different scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac10af4-5ccf-43bb-8aaf-947d01c3fb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tif_folder_path = 's3a://wherobots-benchmark-prod/data/ml/satlas/'\n",
    "files_df = read_raster_table(tif_folder_path, sedona, limit=400)\n",
    "df_raster_input = files_df.withColumn(\n",
    "        \"outdb_raster\", expr(\"RS_FromPath(path)\")\n",
    "    )\n",
    "\n",
    "df_raster_input.cache().count()\n",
    "df_raster_input.show(truncate=False)\n",
    "df_raster_input.createOrReplaceTempView(\"df_raster_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e55013-053f-4de6-b88f-1ca0b31a0ada",
   "metadata": {},
   "source": [
    "### 3: Run Predictions And Visualize Results\n",
    "\n",
    "To run predictions we will specify the MLM model metadata file we saved to `user_mlm_uri`. Predictions can be run with the Raster Inference SQL function [`RS_Segment`](https://docs.wherobots.com/latest/api/wherobots-inference/pythondoc/inference/sql_functions/) or the Python API.\n",
    "\n",
    "Here we generate 300 raster predictions using `RS_Segment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4399933-2da4-4254-bfb3-aea354c3443f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_df = sedona.sql(f\"\"\"\n",
    "SELECT\n",
    "  outdb_raster,\n",
    "  segment_result.*\n",
    "FROM (\n",
    "  SELECT\n",
    "    outdb_raster,\n",
    "    RS_SEGMENT('{user_mlm_uri}', outdb_raster) AS segment_result\n",
    "  FROM\n",
    "    df_raster_input\n",
    ") AS segment_fields\n",
    "\"\"\")\n",
    "\n",
    "predictions_df.cache().count()\n",
    "predictions_df.show()\n",
    "predictions_df.createOrReplaceTempView(\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a2bf5-ea20-400b-8b3a-cc3ea0b7ad88",
   "metadata": {},
   "source": [
    "Now that we've generated predictions using our model over our satellite imagery, we can use the `RS_Segment_To_Geoms` function to extract the geometries indicating the model has identified as possible solar farms. we'll specify the following:\n",
    "\n",
    "* a raster column to use for georeferencing our results\n",
    "* the prediction result from the previous step\n",
    "* our category label \"1\" returned by the model representing Solar Farms and the class map to use for assigning labels to the prediction\n",
    "* a confidence threshold between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad63ec-3f82-42de-835e-b16601de1405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_multipolys = sedona.sql(\"\"\"\n",
    "    WITH t AS (\n",
    "        SELECT RS_SEGMENT_TO_GEOMS(outdb_raster, confidence_array, array(1), class_map, 0.65) result\n",
    "        FROM predictions\n",
    "    )\n",
    "    SELECT result.* FROM t\n",
    "\"\"\")\n",
    "\n",
    "df_multipolys.cache().count()\n",
    "df_multipolys.show()\n",
    "df_multipolys.createOrReplaceTempView(\"multipolygon_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd554156",
   "metadata": {},
   "source": [
    "Since we ran inference across the state of Arizona, many scenes don't contain solar farms and don't have positive detections. Let's filter out scenes without segmentation detections so that we can plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4936d2-723b-4613-bf4e-d1fe3f1f8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_predictions = sedona.sql(\"\"\"\n",
    "    SELECT\n",
    "        element_at(class_name, 1) AS class_name,\n",
    "        cast(element_at(average_pixel_confidence_score, 1) AS double) AS average_pixel_confidence_score,\n",
    "        ST_Collect(geometry) AS merged_geom\n",
    "    FROM\n",
    "        multipolygon_predictions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f5143-d6ac-49db-9d44-ad60eaafefb8",
   "metadata": {},
   "source": [
    "This leaves us with a few predicted solar farm polygons for our 300 satellite image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab528c-51e8-43ee-ba5f-981d50c20f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_predictions = df_merged_predictions.filter(\"ST_IsEmpty(merged_geom) = False\")\n",
    "df_filtered_predictions.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45731d3-eed9-436b-b2cc-095a73882827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8269f80",
   "metadata": {},
   "source": [
    "We'll plot these with SedonaKepler. Compare the satellite basemap with the predictions and see if there's a match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153615b-83e8-4a7b-8f21-efea59a93808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.maps.SedonaKepler import SedonaKepler\n",
    "config = {\n",
    "    'version': 'v1',\n",
    "    'config': {\n",
    "        'mapStyle': {\n",
    "            'styleType': 'dark',\n",
    "            'topLayerGroups': {},\n",
    "            'visibleLayerGroups': {},\n",
    "            'mapStyles': {}\n",
    "        },\n",
    "    }\n",
    "}\n",
    "map = SedonaKepler.create_map(config=config)\n",
    "\n",
    "SedonaKepler.add_df(map, df=df_filtered_predictions, name=\"Solar Farm Detections\")\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952aaa1-9479-4bc5-98bc-461bc604bf48",
   "metadata": {},
   "source": [
    "### wherobots.inference Python API\n",
    "\n",
    "If you prefer python, wherobots.inference offers a module for registering the SQL inference functions as python functions. Below we run the same inference as before with RS_SEGMENT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e631a91-6837-4bfe-a50f-6968eaac6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wherobots.inference.engine.register import create_semantic_segmentation_udfs\n",
    "from pyspark.sql.functions import col\n",
    "rs_segment =  create_semantic_segmentation_udfs(batch_size = 10, sedona=sedona)\n",
    "df = df_raster_input.withColumn(\"segment_result\", rs_segment(user_mlm_uri, col(\"outdb_raster\"))).select(\n",
    "                               \"outdb_raster\",\n",
    "                               col(\"segment_result.confidence_array\").alias(\"confidence_array\"),\n",
    "                               col(\"segment_result.class_map\").alias(\"class_map\")\n",
    "                           )\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f379e56-6fdf-4825-9bb7-024781dea77d",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Bastani, Favyen, Wolters, Piper, Gupta, Ritwik, Ferdinando, Joe, and Kembhavi, Aniruddha. \"SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding.\" *arXiv preprint arXiv:2211.15660* (2023). [https://doi.org/10.48550/arXiv.2211.15660](https://doi.org/10.48550/arXiv.2211.15660)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
